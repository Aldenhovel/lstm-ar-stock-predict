{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f530b2c3-60bb-4cfc-a85a-270a49933081",
   "metadata": {
    "tags": []
   },
   "source": [
    "from utils.Tokenizer import RandomCropTokenizer, MAX_LEN, Tokenizer\n",
    "from utils.StockDataset import StockDataset\n",
    "from utils.DataReader import DataReader\n",
    "from utils.Inference import Inference\n",
    "from utils.Train import train_step, vaild_step\n",
    "from utils.Loss import MyLoss\n",
    "from models.LSTMDecoder import LSTMDecoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbd163-76b5-4a0c-a7bd-a74d889ddd3c",
   "metadata": {},
   "source": [
    "# Check dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee02f9e1-0131-4483-81e3-a0289a7bda17",
   "metadata": {
    "tags": []
   },
   "source": [
    "tk_test = Tokenizer(grid=100, maxlen=MAX_LEN)\n",
    "train_ds = StockDataset(\"data/train\", r\"*/*.yaml\", tokenizer=tk_test)\n",
    "#train_ds.checkds()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3081a3d-86be-4331-8c63-9dcbc6bfc883",
   "metadata": {
    "tags": []
   },
   "source": [
    "tk_train = RandomCropTokenizer(grid=100, maxlen=MAX_LEN, minlen=10)\n",
    "train_ds = StockDataset(\"data/train\", r\"*/*.yaml\", tokenizer=tk_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "'''\n",
    "for b in train_dl:\n",
    "    break\n",
    "b, len(train_ds)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ae15f-1bea-435c-9be6-665239d4038f",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "802b047d-a7e0-4d6e-86f7-710e5a3e8736",
   "metadata": {
    "tags": []
   },
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = LSTMDecoder(vocab_size=102, hidden_dim=512, layer=2, bi=True, device=device)\n",
    "model = torch.load(\"checkpoints/model-pretrained.pt\", map_location=device)\n",
    "model.device = device\n",
    "criterion = nn.KLDivLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213ff9d-3426-4223-88b7-c533302cd440",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea4850d3-97b0-47a7-80b4-0c48756f76f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "for i in range(20):\n",
    "    train_step(model, optimizer, criterion, train_dl, i, device)\n",
    "    if i % 10 == 0:\n",
    "        vaild_step(model, train_dl, i, device)\n",
    "        torch.save(model, \"checkpoints/model-latest.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fbcd06-887a-4e7f-90e5-ed01e68010d2",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad63572c-7c34-488a-a90c-9e56de568f48",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = torch.load(\"checkpoints/model-pretrained.pt\", map_location=device)\n",
    "model.device = device\n",
    "inference = Inference(model=model, device=device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d605e062-0b13-4dd9-88b9-6baf83516987",
   "metadata": {},
   "source": [
    "def plot_future(history, future_3=None, future_5=None, future_10=None, future_greedy=None, max_step=10):\n",
    "    fig = plt.figure(figsize=[10, 5])\n",
    "    xmax = len(history) + max_step\n",
    "    if future_10:\n",
    "        future_min, future_max = 1e9, -1e9\n",
    "        for i in range(len(future_10)):\n",
    "            pred_curve = history + future_10[i][:xmax]\n",
    "            pred_curve = inference.postprocess([pred_curve])[0]\n",
    "            plt.plot(pred_curve, label=\"pred\", color=\"#1b7c3d\")\n",
    "            future_min, future_max = min(future_min, pred_curve[-1]), max(future_max, pred_curve[-1])\n",
    "        vline10 = plt.vlines(len(history)-1 + 10, 0, future_max, linestyles =\"dashed\", colors =\"#1b7c3d\") \n",
    "        plt.vlines(len(history)-1 + 10, 0, future_min, linestyles =\"dashed\", colors =\"#1b7c3d\") \n",
    "    if future_5:\n",
    "        future_min, future_max = 1e9, -1e9\n",
    "        for i in range(len(future_5)):\n",
    "            pred_curve = history + future_5[i][:xmax]\n",
    "            pred_curve = inference.postprocess([pred_curve])[0]\n",
    "            plt.plot(pred_curve, label=\"pred\", color=\"#2b6a99\")   \n",
    "            future_min, future_max = min(future_min, pred_curve[-1]), max(future_max, pred_curve[-1])\n",
    "        vline5 = plt.vlines(len(history)-1 + 5, 0, future_max, linestyles =\"dashed\", colors =\"#2b6a99\")     \n",
    "        plt.vlines(len(history)-1 + 5, 0, future_min, linestyles =\"dashed\", colors =\"#2b6a99\")\n",
    "    if future_3:\n",
    "        future_min, future_max = 1e9, -1e9\n",
    "        for i in range(len(future_3)):\n",
    "            pred_curve = history + future_3[i][:xmax]\n",
    "            pred_curve = inference.postprocess([pred_curve])[0]\n",
    "            plt.plot(pred_curve, label=\"pred\", color=\"#f16c23\")   \n",
    "            future_min, future_max = min(future_min, pred_curve[-1]), max(future_max, pred_curve[-1])\n",
    "        vline3 = plt.vlines(len(history)-1 + 3, 0, future_max, linestyles =\"dashed\", colors =\"#f16c23\")\n",
    "        plt.vlines(len(history)-1 + 3, 0, future_min, linestyles =\"dashed\", colors =\"#f16c23\")\n",
    "        \n",
    "    if future_greedy:\n",
    "        future_min, future_max = 1e9, -1e9\n",
    "        for i in range(len(future_greedy)):\n",
    "            pred_curve = history + future_greedy[i][:xmax]\n",
    "            pred_curve = inference.postprocess([pred_curve])[0]\n",
    "            plt.plot(pred_curve, label=\"pred\", color=\"#f16c23\")   \n",
    "            future_min, future_max = min(future_min, pred_curve[-1]), max(future_max, pred_curve[-1])\n",
    "        vlinegd = plt.vlines(len(history)-1 + len(future_greedy[i][:xmax]), 0, future_max, linestyles =\"dashed\", colors =\"#f16cca\")\n",
    "        plt.vlines(len(history)-1 + len(future_greedy[i][:xmax]), 0, future_min, linestyles =\"dashed\", colors =\"#f16cca\")\n",
    "        \n",
    "    history = inference.postprocess([history])[0]\n",
    "    today = plt.vlines(len(history)-1, 0, history[-1], colors =\"gray\")    \n",
    "    handles, labels = [today], ['today']\n",
    "    if future_3: handles.append(vline3), labels.append('+3days')\n",
    "    if future_5: handles.append(vline5), labels.append('+5days')\n",
    "    if future_10: handles.append(vline10), labels.append('+10days')\n",
    "    if future_greedy: handles.append(vlinegd), labels.append(f'greedy search: +{len(future_greedy[i][:xmax])} days')\n",
    "    plt.legend(handles=handles, labels=labels, loc='best')   \n",
    "    \n",
    "    plt.plot(history, color=\"black\")    \n",
    "    plt.xlim([0, xmax])\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Change %\")\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497f6aa-80f3-4fde-a46e-c7c84b776c42",
   "metadata": {},
   "source": [
    "## Greedy Search"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8fd06b9-2416-4a30-8302-b4ad10d6b00b",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = train_ds.reader.readyaml(\"./data/test/300001.yaml\")\n",
    "    today = data[\"end\"]\n",
    "    seq_raw = data[\"stdchange\"]\n",
    "    seq, seqlen = tk_test.tokenize(seq_raw)\n",
    "    seq = torch.Tensor(seq).long().to(device)\n",
    "    \n",
    "    pred, _, _ = inference.greedy_search(seq.to(device), seqlen, predict_step=30)\n",
    "    seq = seq.cpu().numpy().tolist()\n",
    "    \n",
    "    fig = plot_future(history=seq[1: 1+seqlen], future_greedy=pred, max_step=30)\n",
    "    #fig.savefig('img/300001_gs.svg', format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76603d8-40ad-41f7-9a19-59780b058cc4",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6ab2d88-8585-4d95-b26c-bfb22f50e966",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = train_ds.reader.readyaml(\"./data/test/300001.yaml\")\n",
    "    today = data[\"end\"]\n",
    "    seq_raw = data[\"stdchange\"]\n",
    "    seq, seqlen = tk_test.tokenize(seq_raw)\n",
    "    seq = torch.Tensor(seq).long().to(device)\n",
    "    \n",
    "    preds_3, _, _ = inference.beam_search(seq.to(device), seqlen, beam_size=10, predict_step=3)\n",
    "    preds_5, _, _ = inference.beam_search(seq.to(device), seqlen, beam_size=20, predict_step=5)\n",
    "    preds_10, _, _ = inference.beam_search(seq.to(device), seqlen, beam_size=50, predict_step=10)\n",
    "    seq = seq.cpu().numpy().tolist()\n",
    "    \n",
    "    fig = plot_future(history=seq[1: 1+seqlen], future_3=preds_3, future_5=preds_5, future_10=preds_10, max_step=10)\n",
    "    #fig.savefig('img/300001_bs.svg', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17c3fb-959b-4acc-8d8d-b51390796e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
